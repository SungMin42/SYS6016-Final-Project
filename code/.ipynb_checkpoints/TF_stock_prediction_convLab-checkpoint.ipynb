{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is a script to use deep learning to predict stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import math\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and create Dataset iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_volume</th>\n",
       "      <th>mid_price_log</th>\n",
       "      <th>target</th>\n",
       "      <th>trade_volume_differential</th>\n",
       "      <th>mid_price_log_direction_0_1</th>\n",
       "      <th>mid_price_log_direction_0_2</th>\n",
       "      <th>mid_price_log_direction_0_3</th>\n",
       "      <th>mid_price_log_direction_0_4</th>\n",
       "      <th>mid_price_log_direction_0_5</th>\n",
       "      <th>mid_price_log_direction_0_6</th>\n",
       "      <th>trade_volume_differential_direction_0_1</th>\n",
       "      <th>trade_volume_differential_direction_0_2</th>\n",
       "      <th>trade_volume_differential_direction_0_3</th>\n",
       "      <th>trade_volume_differential_direction_0_4</th>\n",
       "      <th>trade_volume_differential_direction_0_5</th>\n",
       "      <th>trade_volume_differential_direction_0_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57599089</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.491830</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599090</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.550179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599091</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.372644</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599092</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.571578</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599093</th>\n",
       "      <td>4659293</td>\n",
       "      <td>24.424568</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          trade_volume  mid_price_log  target  trade_volume_differential  \\\n",
       "groupid                                                                    \n",
       "57599089       4659293      25.491830       2                        0.0   \n",
       "57599090       4659293      25.550179       0                        0.0   \n",
       "57599091       4659293      25.372644       2                        0.0   \n",
       "57599092       4659293      25.571578       0                        0.0   \n",
       "57599093       4659293      24.424568       0                        0.0   \n",
       "\n",
       "          mid_price_log_direction_0_1  mid_price_log_direction_0_2  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          mid_price_log_direction_0_3  mid_price_log_direction_0_4  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          mid_price_log_direction_0_5  mid_price_log_direction_0_6  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          trade_volume_differential_direction_0_1  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_2  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_3  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_4  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_5  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_6  \n",
       "groupid                                            \n",
       "57599089                                        1  \n",
       "57599090                                        1  \n",
       "57599091                                        1  \n",
       "57599092                                        1  \n",
       "57599093                                        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data (from pkl file probably)\n",
    "\n",
    "#os.getcwd()\n",
    "df = pd.read_pickle('../data/price_level_total_view_2017-01-03_AAPL_grouped_2')\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = df.iloc[:,[1,3,4,5,6,10,11,12]]\n",
    "\n",
    "#dfX = ss.fit_transform(dfX)\n",
    "#dfX = df.iloc[1:,[1,3,4,5,6,10,11,12]]\n",
    "dfy = df.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_to_3d(data, labels, window):\n",
    "    data_n, data_w = data.shape\n",
    "    stride1, stride2 = data.strides\n",
    "    new_len = data_n - window\n",
    "    data3d = as_strided(data, [new_len , window, data_w], strides=[stride1, stride1, stride2])\n",
    "    return(data3d, labels[:len(labels)-window])\n",
    "\n",
    "def flatten_3d(data):\n",
    "    data_n = data.shape[0]\n",
    "    new_width = data.shape[1]*data.shape[2]\n",
    "    \n",
    "    return np.reshape(data, (data_n, new_width))\n",
    "    \n",
    "def split_data(dfX, dfy, train_frac):\n",
    "    \n",
    "    #X = df.iloc[:,0:2]\n",
    "    #y = df.iloc[:,2:3]\n",
    "    X = dfX\n",
    "    y = dfy\n",
    "    n = X.shape[0]\n",
    "    cutoff = np.floor(n * train_frac).astype(int) # total - the number you want to test, which here i'm flooring \n",
    "    #                   (amount you want in training should be 1/10th value the denominator)\n",
    "    # cutoff\n",
    "    \n",
    "    #print(cutoff)\n",
    "\n",
    "    X_train, X_test = (X.iloc[0:cutoff , :] , X.iloc[cutoff: , :] )\n",
    "    #print(X_train.shape, X_test.shape)\n",
    "\n",
    "    y_train, y_test = (y.iloc[0:cutoff].values.ravel() , y.iloc[cutoff:].values.ravel() )\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    X_train = ss.transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "    \n",
    "    \n",
    "#     y_train_temp = np.zeros([y_train.shape[0], 3])\n",
    "#     y_train_temp[np.arange(y_train.shape[0]),y_train] = 1\n",
    "#     y_train = y_train_temp.astype(int)\n",
    "    \n",
    "#     y_test_temp = np.zeros([y_test.shape[0], 3])\n",
    "#     y_test_temp[np.arange(y_test.shape[0]),y_test] = 1\n",
    "#     y_test = y_test_temp.astype(int)\n",
    "\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test sets\n",
    "trainarray,labelarray,testarray,testlabelarray = split_data(dfX, dfy, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data set to use data windows\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "window_size = 6\n",
    "\n",
    "trainarray, labelarray = features_2d_to_3d(np.array(trainarray), np.array(labelarray), window_size)\n",
    "testarray, testlabelarray = features_2d_to_3d(np.array(testarray), np.array(testlabelarray), window_size)\n",
    "\n",
    "labelarray = labelarray[window_size:]\n",
    "testlabelarray = testlabelarray[window_size:]\n",
    "trainarray = trainarray[:(trainarray.shape[0]-window_size),]\n",
    "testarray = testarray[:(testarray.shape[0]-window_size),]\n",
    "\n",
    "# Input\n",
    "height = trainarray.shape[1]\n",
    "width = trainarray.shape[2]\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "trainarray = flatten_3d(trainarray) # don't need to flatten for CNN\n",
    "testarray = flatten_3d(testarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, last batch accuracy: 60.0000%, valid. accuracy: 74.9705%, valid. best loss: 0.996817\n",
      "Epoch 1, last batch accuracy: 46.0000%, valid. accuracy: 74.9273%, valid. best loss: 0.996236\n",
      "Epoch 2, last batch accuracy: 46.0000%, valid. accuracy: 74.9682%, valid. best loss: 0.996236\n",
      "Early stopping!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ae3d1f74aeb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_model_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mrestore_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final accuracy on test set:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Define parameters for the CNN\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for TWO convolutional layers:\n",
    "# Number of feature maps, size of receptive field, stride, and padding\n",
    "conv1_fmaps, conv1_ksize, conv1_stride, conv1_pad = 32, 3, 1, \"SAME\"\n",
    "conv2_fmaps, conv2_ksize, conv2_stride, conv2_pad = 64, 3, 1, \"SAME\"\n",
    "\n",
    "# Define a pooling layer\n",
    "pool3_dropout_rate = 0.25\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "# Define a fully connected layer\n",
    "n_fc1 = 128\n",
    "fc1_dropout_rate = 0.5\n",
    "\n",
    "# Output\n",
    "n_outputs = 3\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Step 2: Set up placeholders for input data\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "# Step 3: Set up the two convolutional layers using tf.layers.conv2d\n",
    "# Hint: arguments would include the parameters defined above, and what activation function to use\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, \n",
    "                         strides=(conv1_stride), padding=conv1_pad, activation=tf.nn.relu,\n",
    "                         name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, \n",
    "                         strides=(conv2_stride), padding=conv2_pad, activation=tf.nn.relu,\n",
    "                         name=\"conv2\")\n",
    "\n",
    "# Step 4: Set up the pooling layer with dropout using tf.nn.max_pool\n",
    "\n",
    "#4-d tensor is [batch, height, width, channel] -- no one does pooling across the batch apparently, so keep that 1 always\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\", name=\"pool\")    \n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * height//2 * width//2])\n",
    "    pool3_flat_drop = tf.layers.dropout(pool3_flat, pool3_dropout_rate, training=training)\n",
    "\n",
    "# Step 5: Set up the fully connected layer using tf.layers.dense\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\n",
    "\n",
    "# Step 6: Calculate final output from the output of the fully connected layer\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits, Y_proba = None, None # Hint: \"Y_proba\" is the softmax output of the \"logits\"\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "# Step 5: Define the optimizer; taking as input (learning_rate) and (loss)\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Step 6: Define the evaluation metric\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct, accuracy = None, None\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "    \n",
    "# Step 7: Initiate\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "# Step 8: Read in data\n",
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "# X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "# y_train = y_train.astype(np.int32)\n",
    "# y_test = y_test.astype(np.int32)\n",
    "# X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "# y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "X_train = trainarray.astype(np.float32).reshape(-1, height*width)\n",
    "y_train = labelarray.astype(np.int32)\n",
    "X_valid = testarray.astype(np.float32).reshape(-1, height*width)\n",
    "y_valid = testlabelarray.astype(np.int32)\n",
    "X_t = X_valid\n",
    "\n",
    "# Step 9: Define some necessary functions\n",
    "def get_model_params():\n",
    "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
    "\n",
    "def restore_model_params(model_params):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Step 10: Define training and evaluation parameters\n",
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "\n",
    "best_loss_val = np.infty\n",
    "check_interval = 500\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 20\n",
    "best_model_params = None\n",
    "\n",
    "# Step 11: Train and evaluate CNN with Early Stopping procedure defined at the very top\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            iteration += 1\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "            if iteration % check_interval == 0:\n",
    "                loss_val = loss.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "                if loss_val < best_loss_val:\n",
    "                    best_loss_val = loss_val\n",
    "                    checks_since_last_progress = 0\n",
    "                    best_model_params = get_model_params()\n",
    "                else:\n",
    "                    checks_since_last_progress += 1\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})/batch_size\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})/44000\n",
    "        print(\"Epoch {}, last batch accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
    "                  epoch, acc_batch * 100, acc_val * 100, best_loss_val))\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    if best_model_params:\n",
    "        restore_model_params(best_model_params)\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "    print(\"Final accuracy on test set:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "p = get_predictions(tep_init, test_n, output)\n",
    "\n",
    "# actual along left, predicted along top\n",
    "confusion_matrix(testlabelarray, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p[:10])\n",
    "print(testlabelarray[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(testlabelarray, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
