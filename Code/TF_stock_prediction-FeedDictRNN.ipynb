{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is a script to use deep learning to predict stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/SYS6016-Final-Project/Code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_volume</th>\n",
       "      <th>mid_price_log</th>\n",
       "      <th>target</th>\n",
       "      <th>trade_volume_differential</th>\n",
       "      <th>mid_price_log_direction_0_1</th>\n",
       "      <th>mid_price_log_direction_0_2</th>\n",
       "      <th>mid_price_log_direction_0_3</th>\n",
       "      <th>mid_price_log_direction_0_4</th>\n",
       "      <th>mid_price_log_direction_0_5</th>\n",
       "      <th>mid_price_log_direction_0_6</th>\n",
       "      <th>trade_volume_differential_direction_0_1</th>\n",
       "      <th>trade_volume_differential_direction_0_2</th>\n",
       "      <th>trade_volume_differential_direction_0_3</th>\n",
       "      <th>trade_volume_differential_direction_0_4</th>\n",
       "      <th>trade_volume_differential_direction_0_5</th>\n",
       "      <th>trade_volume_differential_direction_0_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57599089</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.491830</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599090</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.550179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599091</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.372644</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599092</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.571578</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599093</th>\n",
       "      <td>4659293</td>\n",
       "      <td>24.424568</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          trade_volume  mid_price_log  target  trade_volume_differential  \\\n",
       "groupid                                                                    \n",
       "57599089       4659293      25.491830       2                        0.0   \n",
       "57599090       4659293      25.550179       0                        0.0   \n",
       "57599091       4659293      25.372644       2                        0.0   \n",
       "57599092       4659293      25.571578       0                        0.0   \n",
       "57599093       4659293      24.424568       0                        0.0   \n",
       "\n",
       "          mid_price_log_direction_0_1  mid_price_log_direction_0_2  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          mid_price_log_direction_0_3  mid_price_log_direction_0_4  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          mid_price_log_direction_0_5  mid_price_log_direction_0_6  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          trade_volume_differential_direction_0_1  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_2  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_3  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_4  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_5  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_6  \n",
       "groupid                                            \n",
       "57599089                                        1  \n",
       "57599090                                        1  \n",
       "57599091                                        1  \n",
       "57599092                                        1  \n",
       "57599093                                        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data (from pkl file probably)\n",
    "\n",
    "#os.getcwd()\n",
    "df = pd.read_pickle('../data/price_level_total_view_2017-01-03_AAPL_grouped_2')\n",
    "df.head()\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 0, 1, 2, 1, 0, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "# temp = features_2d_to_3d(trainarray, labelarray, 10)\n",
    "# np.array_split(temp[0], 10)[0].shape\n",
    "#labelarray.strides\n",
    "#print(labelarray[0:20])\n",
    "np.reshape(as_strided(labelarray, [labelarray.shape[0]-10, 10], strides=[4,4]), [-1,10])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_to_3d(data, labels, window):\n",
    "    data_n, data_w = data.shape\n",
    "    stride1, stride2 = data.strides\n",
    "    #l_stride1, l_stride2 = labels.strides\n",
    "    l_stride1 = labels.strides[0]\n",
    "    new_len = data_n - window\n",
    "    data3d = as_strided(data, [new_len, window, data_w], strides=[stride1, stride1, stride2])\n",
    "    #labels3d = as_strided(labels, [new_len, window, 3], strides=[l_stride1, l_stride1, l_stride2])\n",
    "    labels3d = as_strided(labels, [new_len, window], strides=[l_stride1, l_stride1])\n",
    "    return data3d, labels3d\n",
    "\n",
    "def flatten_3d(data):\n",
    "    data_n = data.shape[0]\n",
    "    new_width = data.shape[1]*data.shape[2]\n",
    "    \n",
    "    return np.reshape(data, (data_n, new_width)) # flesh this function out\n",
    "    \n",
    "def split_data(dfX, dfy, train_frac):\n",
    "    X = dfX\n",
    "    y = dfy\n",
    "    n = X.shape[0]\n",
    "    cutoff = np.floor(n * train_frac).astype(int) # total - the number you want to test, which here i'm flooring\n",
    "    #                   (amount you want in training should be 1/10th value the denominator)\n",
    "    # cutoff\n",
    "\n",
    "    X_train, X_test = (X.iloc[0:cutoff , :] , X.iloc[cutoff: , :] )\n",
    "    y_train, y_test = (y.iloc[0:cutoff].values.ravel() , y.iloc[cutoff:].values.ravel() )\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    X_train = ss.transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def batch_data(data, labels, batch_size, n_steps):\n",
    "    windowed_x, windowed_y = features_2d_to_3d(data, labels, n_steps)\n",
    "    \n",
    "    t_steps = data.shape[0]\n",
    "    width = data.shape[1]\n",
    "    n_batches = t_steps // batch_size\n",
    "    remainder = t_steps - (n_batches * batch_size)\n",
    "    new_len = t_steps - remainder\n",
    "    \n",
    "    windowed_x = windowed_x[:new_len]\n",
    "    windowed_y = windowed_y[:new_len]\n",
    "    \n",
    "    x_batches = np.reshape(windowed_x, [-1, batch_size, n_steps, width])\n",
    "#     y_batches = np.reshape(windowed_y, [-1, batch_size, n_steps, 3])\n",
    "#     y_batches = y_batches[:,:,n_steps-1,:]\n",
    "    y_batches = np.reshape(windowed_y, [-1, batch_size, n_steps])\n",
    "    y_batches = y_batches[:,:,n_steps-1]\n",
    "    \n",
    "    print(x_batches.shape)\n",
    "    print(y_batches.shape)\n",
    "\n",
    "    return x_batches, y_batches, n_batches, new_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2388, 100)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#x, y = features_2d_to_3d(trainarray, labelarray, 10)\n",
    "#y = y[:238800]\n",
    "#print(y[1])\n",
    "#print(y[0:10,9])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(n_steps, batch_size):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Define parameters for the RNN\n",
    "    n_inputs = 14\n",
    "    n_neurons = 100\n",
    "    n_outputs = 3\n",
    "\n",
    "    # Set up placeholders for input data\n",
    "    X = tf.placeholder(tf.float32, [batch_size, n_steps, n_inputs])\n",
    "    labels = tf.placeholder(tf.int32, [batch_size])\n",
    "    \n",
    "    with tf.name_scope(\"rnn\"):\n",
    "        \n",
    "        cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu), output_size=n_outputs)\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "        \n",
    "    with tf.name_scope(\"output\"):\n",
    "        logits = tf.layers.dense(final_state, n_outputs, name=\"output\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "        \n",
    "        print(logits.shape)\n",
    "        print(labels.shape)\n",
    "\n",
    "    # Define the optimizer; taking as input (learning_rate) and (loss)\n",
    "    with tf.name_scope(\"train\"):\n",
    "        learning_rate = 0.001\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Step 6: Define the evaluation metric\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "\n",
    "    # Step 7: Initiate\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Summaries\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        value_ = tf.placeholder(shape=[], name='summary_placeholder', dtype=tf.float32)\n",
    "        #ta = tf.placeholder(shape=[], name='train_accuracy_p', dtype=tf.float32)\n",
    "        #vl = tf.placeholder(shape=[], name='val_loss_p', dtype=tf.float32)\n",
    "        #va = tf.placeholder(shape=[], name='val_accuracy_p', dtype=tf.float32)\n",
    "        tl = tf.summary.scalar('train_loss', value_)\n",
    "        ta = tf.summary.scalar('train_accuracy', value_)\n",
    "        vl = tf.summary.scalar('val_loss', value_)\n",
    "        va = tf.summary.scalar('val_accuracy', value_)\n",
    "        #merged = tf.summary.merge_all()\n",
    "    \n",
    "    return training_op, loss, accuracy, init, X, labels, Y_proba, value_, tl, ta, vl, va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns and scale data\n",
    "dfX = df.reset_index().iloc[:,5:]  #np.delete(np.arange(16), [0,2])\n",
    "dfX2 = df.reset_index().iloc[:,[2,4]]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(dfX2)\n",
    "dfX2 = ss.transform(dfX2)\n",
    "\n",
    "dfX2 = pd.DataFrame(data=dfX2, columns= ['mid_price_log', 'trade_volume_differential'])\n",
    "dfX = pd.merge(dfX2, dfX, left_index=True, right_index=True)\n",
    "\n",
    "dfy = df.reset_index().iloc[:,3]\n",
    "\n",
    "# Create train/test sets\n",
    "trainarray,labelarray,testarray,testlabelarray = split_data(dfX, dfy, 0.8)\n",
    "\n",
    "# try one-hot encoding labels\n",
    "# labelarray = labelarray.reshape(-1)\n",
    "# labelarray = np.eye(3, dtype=np.int32)[labelarray]\n",
    "\n",
    "# testlabelarray = testlabelarray.reshape(-1)\n",
    "# testlabelarray = np.eye(3, dtype=np.int32)[testlabelarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238845,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2388, 100, 10, 14)\n",
      "(2388, 100)\n",
      "(597, 100, 10, 14)\n",
      "(597, 100)\n"
     ]
    }
   ],
   "source": [
    "# transform data set to use data windows\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "window_size = 10\n",
    "batch_size = 100\n",
    "x, y, n_batches, n_train = batch_data(trainarray, labelarray, batch_size, window_size)\n",
    "x_val, y_val, n_batches_val, n_test  = batch_data(testarray, testlabelarray, batch_size, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    115967\n",
       "2     61580\n",
       "0     61298\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labelarray).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random labels if we want to test accuracy\n",
    "\n",
    "# labelarray = np.asarray([np.random.randint(0,3) for i in np.arange(labelarray.shape[0])])\n",
    "# testlabelarray = np.asarray([np.random.randint(0,3) for i in np.arange(testlabelarray.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Create datasets & model\n",
    "\n",
    "training_op, loss, accuracy, init, X, labels, Y_proba, value_, tl, ta, vl, va = create_rnn_model(window_size, batch_size)\n",
    "\n",
    "#temp = create_datasets(trainarray, labelarray, testarray, testlabelarray, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('../tf_graphs/', tf.get_default_graph())\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        for b in range(n_batches):\n",
    "            X_batch, y_batch = x[b], y[b]\n",
    "            _, loss_value, acc_value = sess.run([training_op, loss, accuracy], feed_dict={X: X_batch, labels: y_batch})\n",
    "            train_loss += loss_value\n",
    "            train_accuracy += acc_value\n",
    "        train_accuracy = train_accuracy/n_train\n",
    "        \n",
    "        s = sess.run(ta, feed_dict={value_: train_accuracy})\n",
    "        writer.add_summary(s, epoch)\n",
    "        s = sess.run(tl, feed_dict={value_: train_loss})\n",
    "        writer.add_summary(s, epoch)\n",
    "        \n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        for b in range(n_batches_val):\n",
    "            X_batch, y_batch = x_val[b], y_val[b]       \n",
    "            loss_value, acc_value = sess.run([loss, accuracy], feed_dict={X: X_batch, labels: y_batch})\n",
    "            val_loss += loss_value\n",
    "            val_accuracy += acc_value\n",
    "        val_accuracy = val_accuracy/n_test\n",
    "\n",
    "        s = sess.run(va, feed_dict={value_: val_accuracy})\n",
    "        writer.add_summary(s, epoch)\n",
    "        s = sess.run(vl, feed_dict={value_: val_loss})\n",
    "        writer.add_summary(s, epoch)\n",
    "        \n",
    "        print(\"Epoch: {}, Train acc: {:.4f}, Train loss: {:.4f}, Val acc: {:.4f}, Val loss: {:.4f}\".format(epoch, train_accuracy, train_loss, val_accuracy, val_loss))\n",
    "\n",
    "    saver.save(sess, \"./my_time_series_model\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_time_series_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.91116119e-03, 6.68064475e-01, 3.26024324e-01],\n",
       "       [6.75973017e-03, 6.81333899e-01, 3.11906397e-01],\n",
       "       [5.25943398e-01, 4.69875723e-01, 4.18093754e-03],\n",
       "       [3.78358923e-02, 5.55407763e-01, 4.06756312e-01],\n",
       "       [3.66825350e-02, 5.74981034e-01, 3.88336360e-01],\n",
       "       [3.13772149e-02, 6.10367656e-01, 3.58255118e-01],\n",
       "       [5.26671767e-01, 4.67410237e-01, 5.91800455e-03],\n",
       "       [4.36438262e-01, 5.47167242e-01, 1.63944922e-02],\n",
       "       [4.58892494e-01, 5.22048950e-01, 1.90585628e-02],\n",
       "       [4.25328434e-01, 5.50922036e-01, 2.37495657e-02],\n",
       "       [4.00651805e-03, 5.26340127e-01, 4.69653368e-01],\n",
       "       [7.88349845e-03, 6.29697263e-01, 3.62419277e-01],\n",
       "       [3.90948474e-01, 6.01414680e-01, 7.63688888e-03],\n",
       "       [7.04058772e-03, 5.61468661e-01, 4.31490749e-01],\n",
       "       [7.27534294e-03, 6.55013263e-01, 3.37711394e-01],\n",
       "       [4.91841495e-01, 5.02743125e-01, 5.41539071e-03],\n",
       "       [4.43212360e-01, 5.50728500e-01, 6.05906779e-03],\n",
       "       [5.79179358e-03, 5.96456826e-01, 3.97751391e-01],\n",
       "       [5.62964613e-03, 6.74497902e-01, 3.19872379e-01],\n",
       "       [6.25840796e-04, 4.14663136e-01, 5.84711015e-01],\n",
       "       [2.92493045e-01, 6.77590668e-01, 2.99162511e-02],\n",
       "       [3.31523567e-01, 6.22926474e-01, 4.55499776e-02],\n",
       "       [1.21677667e-03, 4.18624699e-01, 5.80158532e-01],\n",
       "       [4.74124640e-01, 5.19792020e-01, 6.08330453e-03],\n",
       "       [4.23640847e-01, 5.67828119e-01, 8.53110291e-03],\n",
       "       [4.62327629e-01, 5.23475647e-01, 1.41966902e-02],\n",
       "       [1.87004190e-02, 6.16701663e-01, 3.64597887e-01],\n",
       "       [4.71374601e-01, 5.25105596e-01, 3.51977861e-03],\n",
       "       [8.47422238e-03, 5.21688581e-01, 4.69837219e-01],\n",
       "       [5.80650289e-03, 6.33616447e-01, 3.60577017e-01],\n",
       "       [4.57914561e-01, 5.38431942e-01, 3.65344179e-03],\n",
       "       [9.37190093e-03, 5.61616123e-01, 4.29012001e-01],\n",
       "       [1.31299365e-02, 6.82000518e-01, 3.04869592e-01],\n",
       "       [5.03663540e-01, 4.91590649e-01, 4.74583730e-03],\n",
       "       [4.41901684e-01, 5.52438974e-01, 5.65935159e-03],\n",
       "       [5.81962289e-03, 5.93641460e-01, 4.00538892e-01],\n",
       "       [5.59084816e-03, 6.72537863e-01, 3.21871251e-01],\n",
       "       [5.53751737e-03, 6.71757281e-01, 3.22705209e-01],\n",
       "       [5.61831612e-03, 6.83156312e-01, 3.11225444e-01],\n",
       "       [6.02149079e-03, 6.81553066e-01, 3.12425464e-01],\n",
       "       [5.30763924e-01, 4.63868558e-01, 5.36748534e-03],\n",
       "       [1.06936572e-02, 6.24130487e-01, 3.65175843e-01],\n",
       "       [6.98905857e-03, 6.84170842e-01, 3.08840007e-01],\n",
       "       [6.88168593e-03, 7.10549891e-01, 2.82568395e-01],\n",
       "       [5.39887309e-01, 4.56164926e-01, 3.94778326e-03],\n",
       "       [1.09759746e-02, 6.08011067e-01, 3.81013036e-01],\n",
       "       [5.12614846e-01, 4.81854081e-01, 5.53107588e-03],\n",
       "       [4.49297577e-01, 5.43321729e-01, 7.38068111e-03],\n",
       "       [6.15388202e-03, 5.81093550e-01, 4.12752539e-01],\n",
       "       [7.49130175e-03, 6.58474326e-01, 3.34034383e-01],\n",
       "       [4.59793419e-01, 5.34526706e-01, 5.67988446e-03],\n",
       "       [8.85323714e-03, 5.57659984e-01, 4.33486819e-01],\n",
       "       [4.53965068e-01, 5.40598214e-01, 5.43676224e-03],\n",
       "       [7.64704030e-03, 5.58341324e-01, 4.34011638e-01],\n",
       "       [5.05726993e-01, 4.90397692e-01, 3.87533195e-03],\n",
       "       [7.47928116e-03, 5.64947128e-01, 4.27573591e-01],\n",
       "       [5.10847056e-03, 6.43571198e-01, 3.51320386e-01],\n",
       "       [4.68294024e-01, 5.25597572e-01, 6.10840181e-03],\n",
       "       [4.38943446e-01, 5.54856956e-01, 6.19964208e-03],\n",
       "       [6.49237167e-03, 6.01506591e-01, 3.92001003e-01],\n",
       "       [6.99759927e-03, 6.64873064e-01, 3.28129321e-01],\n",
       "       [4.83710378e-01, 5.12281239e-01, 4.00842493e-03],\n",
       "       [8.52472801e-03, 5.66796720e-01, 4.24678564e-01],\n",
       "       [5.65947080e-03, 6.71520531e-01, 3.22820008e-01],\n",
       "       [4.96248811e-01, 4.98660237e-01, 5.09096356e-03],\n",
       "       [1.14043169e-02, 5.87155700e-01, 4.01439905e-01],\n",
       "       [7.77851511e-03, 6.68455720e-01, 3.23765785e-01],\n",
       "       [5.50169637e-03, 6.66324019e-01, 3.28174323e-01],\n",
       "       [7.62154628e-03, 6.85722053e-01, 3.06656390e-01],\n",
       "       [5.19630075e-01, 4.75671649e-01, 4.69823321e-03],\n",
       "       [4.76064384e-01, 5.16074717e-01, 7.86094088e-03],\n",
       "       [8.43530241e-03, 6.16310954e-01, 3.75253767e-01],\n",
       "       [8.07905011e-03, 6.74974859e-01, 3.16946000e-01],\n",
       "       [4.94813174e-01, 5.00544608e-01, 4.64225840e-03],\n",
       "       [9.08846967e-03, 5.67156315e-01, 4.23755258e-01],\n",
       "       [6.13450538e-03, 6.69937849e-01, 3.23927671e-01],\n",
       "       [4.99539554e-01, 4.95536953e-01, 4.92349779e-03],\n",
       "       [1.13630006e-02, 5.86199522e-01, 4.02437478e-01],\n",
       "       [7.74959056e-03, 6.68190300e-01, 3.24060082e-01],\n",
       "       [5.51429205e-03, 6.65376186e-01, 3.29109550e-01],\n",
       "       [7.56057072e-03, 6.85658097e-01, 3.06781322e-01],\n",
       "       [5.19479513e-01, 4.75810617e-01, 4.70986171e-03],\n",
       "       [1.00690704e-02, 6.05288982e-01, 3.84641886e-01],\n",
       "       [2.22446546e-02, 6.66246593e-01, 3.11508805e-01],\n",
       "       [3.47768478e-02, 6.18351996e-01, 3.46871197e-01],\n",
       "       [2.33432241e-02, 6.48227572e-01, 3.28429222e-01],\n",
       "       [5.41106939e-01, 4.51905817e-01, 6.98723411e-03],\n",
       "       [5.98433800e-03, 5.85937500e-01, 4.08078223e-01],\n",
       "       [1.00400439e-02, 7.01569855e-01, 2.88390130e-01],\n",
       "       [7.86783546e-03, 7.02208877e-01, 2.89923340e-01],\n",
       "       [8.74080043e-03, 7.32300222e-01, 2.58958995e-01],\n",
       "       [5.35248816e-01, 4.61385965e-01, 3.36524751e-03],\n",
       "       [1.18011953e-02, 6.05697870e-01, 3.82500887e-01],\n",
       "       [9.18596517e-03, 6.69787526e-01, 3.21026444e-01],\n",
       "       [2.28566807e-02, 6.64346576e-01, 3.12796712e-01],\n",
       "       [4.88446116e-01, 5.02980888e-01, 8.57303105e-03],\n",
       "       [4.54280436e-01, 5.34523845e-01, 1.11957109e-02],\n",
       "       [1.29108010e-02, 4.95881438e-01, 4.91207719e-01],\n",
       "       [5.08865178e-01, 4.72203135e-01, 1.89316887e-02],\n",
       "       [4.45514219e-03, 4.77692872e-01, 5.17852008e-01]], dtype=float32)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:                          \n",
    "    saver.restore(sess, \"./my_time_series_model\")\n",
    "\n",
    "    X_new = x_val\n",
    "    y_pred = sess.run(Y_proba, feed_dict={X: X_new[0]})\n",
    "    \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
