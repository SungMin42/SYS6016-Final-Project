{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is a script to use deep learning to predict stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and create Dataset iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in data (from pkl file probably)\n",
    "\n",
    "#os.getcwd()\n",
    "df = pd.read_pickle('../data/price_level_total_view_2017-01-03_AAPL_grouped')\n",
    "df.head()\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary object to develop to\n",
    "trainarray = np.zeros([10,5,2], dtype=float) #[n_rows, window_size, features]\n",
    "labelarray = np.zeros([10,2], dtype=float) # [n_rows, output_classes]\n",
    "\n",
    "testarray = np.zeros([10,5,2], dtype=float) #[n_rows, window_size, features]\n",
    "testlabelarray = np.zeros([10,2], dtype=float) # [n_rows, output_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_to_3d(data, labels, window):\n",
    "    data_n, data_w = data.shape\n",
    "    stride1, stride2 = data.strides\n",
    "    new_len = data_n - window\n",
    "    data3d = as_strided(data, [new_len , window, data_w], strides=[stride1, stride1, stride2])\n",
    "    return(data3d, labels[:len(labels)-window])\n",
    "\n",
    "def flatten_3d(data):\n",
    "    data_n = data.shape[0]\n",
    "    new_width = data.shape[1]*data.shape[2]\n",
    "    \n",
    "    return np.reshape(data, (data_n, new_width)) # flesh this function out\n",
    "    \n",
    "def split_data(df, train_frac):\n",
    "    n = X.shape[0]\n",
    "    cutoff = math.round(n * train_frac) # total - the number you want to test, which here i'm flooring \n",
    "    #                   (amount you want in training should be 1/10th value the denominator)\n",
    "    # cutoff\n",
    "\n",
    "    X_train, X_test = (X.iloc[0:cutoff , :] , X.iloc[cutoff: , :] )\n",
    "\n",
    "    y_train, y_test = (y.iloc[0:cutoff , :].values.ravel() , y.iloc[cutoff: , :].values.ravel() )\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(trainarray, labelarray, testarray, testlabelarray, batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    train_n = trainarray.shape[0]\n",
    "    test_n = testarray.shape[0]\n",
    "\n",
    "    with tf.name_scope(\"dataset\"):\n",
    "        training_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(trainarray, tf.float32),\n",
    "                    tf.cast(labelarray, tf.int32)\n",
    "                )\n",
    "            ).shuffle(buffer_size=2*train_n).batch(batch_size) # multiply by 2 if using accuracy calc\n",
    "        )\n",
    "\n",
    "        test_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(testarray, tf.float32),\n",
    "                    tf.cast(testlabelarray, tf.int32)\n",
    "                )\n",
    "            )\n",
    "        ).shuffle(buffer_size=2*test_n).batch(batch_size)\n",
    "\n",
    "    with tf.name_scope(\"iterator\"):\n",
    "        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n",
    "        features, labels = iterator.get_next()\n",
    "        train_init = iterator.make_initializer(training_dataset) # initializer for train_data\n",
    "        test_init = iterator.make_initializer(test_dataset) # initializer for train_data\n",
    "\n",
    "    return features, labels, train_init, test_init, trainarray.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(features, labels, n_inputs, n_outputs=3):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "        hidden1 = tf.layers.dense(features, n_inputs, name=\"hidden1\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer1)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_inputs, name=\"hidden2\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer2)\n",
    "\n",
    "        #max1 = tf.contrib.layers.maxout(hidden1, n_inputs//2)\n",
    "\n",
    "    #     dropout1 = tf.nn.dropout(hidden1, keep_prob)\n",
    "    #     hidden2 = tf.layers.dense(dropout1, n_hidden2, name=\"hidden2\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer2)\n",
    "    #     dropout2 = tf.nn.dropout(hidden2, keep_prob)\n",
    "    #     hidden3 = tf.layers.dense(dropout2, n_hidden3, name=\"hidden3\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer3)\n",
    "    #     dropout3 = tf.nn.dropout(hidden3, keep_prob)\n",
    "    #     hidden4 = tf.layers.dense(dropout3, n_hidden4, name=\"hidden4\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout4 = tf.nn.dropout(hidden4, keep_prob)\n",
    "    #     hidden5 = tf.layers.dense(dropout4, n_hidden5, name=\"hidden5\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout5 = tf.nn.dropout(hidden5, keep_prob)\n",
    "    #     hidden6 = tf.layers.dense(dropout5, n_hidden5, name=\"hidden6\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout6 = tf.nn.dropout(hidden6, keep_prob)\n",
    "    #     hidden7 = tf.layers.dense(dropout6, n_hidden5, name=\"hidden7\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout7 = tf.nn.dropout(hidden7, keep_prob)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")#, kernel_regularizer=regularizer5)\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    #     l2_loss = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "    #     loss += l2_loss\n",
    "        loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "    learning_rate = 0.002\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "        accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    with tf.name_scope(\"predict\"):\n",
    "        output = tf.nn.softmax(logits)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return training_op, output, loss, accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py\n",
    "# will need to set the \n",
    "\n",
    "with tf.name_scope('convolution')\n",
    "\n",
    "def create_convolutional_layer(input_data, num_input_channels, conv_filter_size, num_filters = 100):\n",
    "    \n",
    "    with tf.name_scope('convolution':\n",
    "        kernel = _variable_with_weight_decay('weights',  \n",
    "\n",
    "                                         shape=[, 5, 3, 64],\n",
    "\n",
    "                                         stddev=5e-2,\n",
    "\n",
    "                                         wd=None)\n",
    "\n",
    "        conv = tf.nn.conv1d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    _activation_summary(conv1)\n",
    "\n",
    "def create_cnn_model(features, labels, n_inputs, n_outputs=3):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "        hidden1 = tf.layers.dense(features, n_inputs, name=\"hidden1\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer1)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_inputs, name=\"hidden2\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer2)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")#, kernel_regularizer=regularizer5)\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    #     l2_loss = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "    #     loss += l2_loss\n",
    "        loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "    learning_rate = 0.002\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "        accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    with tf.name_scope(\"predict\"):\n",
    "        output = tf.nn.softmax(logits)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return training_op, output, loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test sets\n",
    "trainarray,labelarray,testarray,testlabelarray = split_data(df, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data set to use data windows\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "trainarray, labelarray = features_2d_to_3d(np.array(trainarray), np.array(labelarray), 5)\n",
    "testarray, testlabelarray = features_2d_to_3d(np.array(testarray), np.array(testlabelarray), 5)\n",
    "# trainarray = flatten_3d(trainarray) # don't need to flatten for CNN\n",
    "# testarray = flatten_3d(testarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets & model\n",
    "\n",
    "n_inputs = trainarray.shape[1]\n",
    "\n",
    "features, labels, tr_init, te_init, n_inputs = create_datasets(trainarray, labelarray, testarray, testlabelarray, batch_size)\n",
    "training_op, output, loss, accuracy = create_model(features, labels, n_inputs, n_outputs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training\n",
    "batch_size = 100\n",
    "train_n = trainarray.shape[0]\n",
    "test_n = testarray.shape[0]\n",
    "n_batches = train_n // batch_size\n",
    "n_batches_test = test_n // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "columns = ['t-plus', 'loss', 'accuracy', 'test_loss', 'test_accuracy']\n",
    "summaries = pd.DataFrame(np.zeros([n_epochs,5], dtype=float), columns=columns)\n",
    "run_name = 'model1'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #writer.add_graph(sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tot_batches_run = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(tr_init) # drawing samples from train_data\n",
    "        tot_loss = 0\n",
    "        for i in range(n_batches):\n",
    "            try:\n",
    "                _, loss_value = sess.run([training_op, loss]) # , feed_dict={keep_prob : 0.75} # for dropout only\n",
    "                tot_loss += loss_value\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"out of range on iter {}\".format(i))\n",
    "                break\n",
    "        \n",
    "        \n",
    "        # Now gauge training accuracy\n",
    "        sess.run(tr_init) # drawing samples from test_data\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch = sess.run(accuracy) # , feed_dict={keep_prob : 1} # for dropout only\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        tr_acc = total_correct_preds/train_n\n",
    "        \n",
    "        # Now get testing loss\n",
    "        sess.run(te_init) # drawing samples from test_data\n",
    "        test_tot_loss = 0\n",
    "        for i in range(n_batches_test):\n",
    "            try:\n",
    "                loss_value = sess.run([loss]) # , feed_dict={keep_prob : 0.75} # for dropout only\n",
    "                test_tot_loss += loss_value[0]\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"out of range on iter {}\".format(i))\n",
    "                break\n",
    "        \n",
    "        # Now gauge testing accuracy\n",
    "        sess.run(te_init) # drawing samples from test_data\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch = sess.run(accuracy) # , feed_dict={keep_prob : 1} # for dropout only\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        te_acc = total_correct_preds/test_n\n",
    "\n",
    "        \n",
    "        epoch_time = time.time()\n",
    "        print(\"Epoch: {}, Train_Loss: {:.4f}, Test_Loss: {:.4f}, Train_Accuracy: {:.4f}, Test_Accuracy: {:.4f}\"\\\n",
    "              .format(epoch, tot_loss, test_tot_loss, tr_acc, te_acc))\n",
    "        cum_time = epoch_time - start_time\n",
    "        summaries.iloc[epoch,:] = cum_time, tot_loss, test_tot_loss, tr_acc, te_acc\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
