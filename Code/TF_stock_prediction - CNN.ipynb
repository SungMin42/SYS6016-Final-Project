{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is a script to use deep learning to predict stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import math\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and create Dataset iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_volume</th>\n",
       "      <th>mid_price_log</th>\n",
       "      <th>target</th>\n",
       "      <th>trade_volume_differential</th>\n",
       "      <th>mid_price_log_direction_0_1</th>\n",
       "      <th>mid_price_log_direction_0_2</th>\n",
       "      <th>mid_price_log_direction_0_3</th>\n",
       "      <th>mid_price_log_direction_0_4</th>\n",
       "      <th>mid_price_log_direction_0_5</th>\n",
       "      <th>mid_price_log_direction_0_6</th>\n",
       "      <th>trade_volume_differential_direction_0_1</th>\n",
       "      <th>trade_volume_differential_direction_0_2</th>\n",
       "      <th>trade_volume_differential_direction_0_3</th>\n",
       "      <th>trade_volume_differential_direction_0_4</th>\n",
       "      <th>trade_volume_differential_direction_0_5</th>\n",
       "      <th>trade_volume_differential_direction_0_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57599089</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.491830</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599090</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.550179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599091</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.372644</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599092</th>\n",
       "      <td>4659293</td>\n",
       "      <td>25.571578</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57599093</th>\n",
       "      <td>4659293</td>\n",
       "      <td>24.424568</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          trade_volume  mid_price_log  target  trade_volume_differential  \\\n",
       "groupid                                                                    \n",
       "57599089       4659293      25.491830       2                        0.0   \n",
       "57599090       4659293      25.550179       0                        0.0   \n",
       "57599091       4659293      25.372644       2                        0.0   \n",
       "57599092       4659293      25.571578       0                        0.0   \n",
       "57599093       4659293      24.424568       0                        0.0   \n",
       "\n",
       "          mid_price_log_direction_0_1  mid_price_log_direction_0_2  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          mid_price_log_direction_0_3  mid_price_log_direction_0_4  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          mid_price_log_direction_0_5  mid_price_log_direction_0_6  \\\n",
       "groupid                                                              \n",
       "57599089                            2                            2   \n",
       "57599090                            2                            2   \n",
       "57599091                            0                            0   \n",
       "57599092                            2                            2   \n",
       "57599093                            0                            0   \n",
       "\n",
       "          trade_volume_differential_direction_0_1  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_2  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_3  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_4  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_5  \\\n",
       "groupid                                             \n",
       "57599089                                        1   \n",
       "57599090                                        1   \n",
       "57599091                                        1   \n",
       "57599092                                        1   \n",
       "57599093                                        1   \n",
       "\n",
       "          trade_volume_differential_direction_0_6  \n",
       "groupid                                            \n",
       "57599089                                        1  \n",
       "57599090                                        1  \n",
       "57599091                                        1  \n",
       "57599092                                        1  \n",
       "57599093                                        1  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data (from pkl file probably)\n",
    "\n",
    "#os.getcwd()\n",
    "df = pd.read_pickle('../data/price_level_total_view_2017-01-03_AAPL_grouped_2')\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = df.iloc[:,[1,3,4,5,6,10,11,12]]\n",
    "\n",
    "#dfX = ss.fit_transform(dfX)\n",
    "#dfX = df.iloc[1:,[1,3,4,5,6,10,11,12]]\n",
    "dfy = df.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298557, 8)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=dfy.shape[0]\n",
    "# cutoff = np.floor(n * 0.8).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_1, _2 = (dfy.iloc[0:cutoff , :].values.ravel() , dfy.iloc[cutoff: , :].values.ravel() )\n",
    "#_1, _2 = (dfy.iloc[0:cutoff].values.ravel() , dfy.iloc[cutoff:].values.ravel() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary object to develop to\n",
    "trainarray = np.zeros([10,5,2], dtype=float) #[n_rows, window_size, features]\n",
    "labelarray = np.zeros([10,2], dtype=float) # [n_rows, output_classes]\n",
    "\n",
    "testarray = np.zeros([10,5,2], dtype=float) #[n_rows, window_size, features]\n",
    "testlabelarray = np.zeros([10,2], dtype=float) # [n_rows, output_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_to_3d(data, labels, window):\n",
    "    data_n, data_w = data.shape\n",
    "    stride1, stride2 = data.strides\n",
    "    new_len = data_n - window\n",
    "    data3d = as_strided(data, [new_len , window, data_w], strides=[stride1, stride1, stride2])\n",
    "    return(data3d, labels[:len(labels)-window])\n",
    "\n",
    "def flatten_3d(data):\n",
    "    data_n = data.shape[0]\n",
    "    new_width = data.shape[1]*data.shape[2]\n",
    "    \n",
    "    return np.reshape(data, (data_n, new_width))\n",
    "    \n",
    "def split_data(dfX, dfy, train_frac):\n",
    "    \n",
    "    #X = df.iloc[:,0:2]\n",
    "    #y = df.iloc[:,2:3]\n",
    "    X = dfX\n",
    "    y = dfy\n",
    "    n = X.shape[0]\n",
    "    cutoff = np.floor(n * train_frac).astype(int) # total - the number you want to test, which here i'm flooring \n",
    "    #                   (amount you want in training should be 1/10th value the denominator)\n",
    "    # cutoff\n",
    "    \n",
    "    #print(cutoff)\n",
    "\n",
    "    X_train, X_test = (X.iloc[0:cutoff , :] , X.iloc[cutoff: , :] )\n",
    "    #print(X_train.shape, X_test.shape)\n",
    "\n",
    "    y_train, y_test = (y.iloc[0:cutoff].values.ravel() , y.iloc[cutoff:].values.ravel() )\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    X_train = ss.transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(trainarray, labelarray, testarray, testlabelarray, batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    train_n = trainarray.shape[0]\n",
    "    test_n = testarray.shape[0]\n",
    "    print(train_n, test_n)                \n",
    "\n",
    "    with tf.name_scope(\"dataset\"):\n",
    "        training_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(trainarray, tf.float32),\n",
    "                    tf.cast(labelarray, tf.int32)\n",
    "                )\n",
    "            ).shuffle(buffer_size=2*train_n).batch(batch_size) # multiply by 2 if using accuracy calc\n",
    "        )\n",
    "\n",
    "        test_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(testarray, tf.float32),\n",
    "                    tf.cast(testlabelarray, tf.int32)\n",
    "                )\n",
    "            )\n",
    "        ).shuffle(buffer_size=2*test_n).batch(batch_size)\n",
    "        \n",
    "        test_pred_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(testarray, tf.float32),\n",
    "                    tf.cast(testlabelarray, tf.int32)\n",
    "                )\n",
    "            )\n",
    "        ).batch(1)\n",
    "\n",
    "    with tf.name_scope(\"iterator\"):\n",
    "        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n",
    "        features, labels = iterator.get_next()\n",
    "        train_init = iterator.make_initializer(training_dataset) # initializer for train_data\n",
    "        test_init = iterator.make_initializer(test_dataset) # initializer for train_data\n",
    "        test_pred_init = iterator.make_initializer(test_pred_dataset) # initializer for train_data\n",
    "\n",
    "    return features, labels, train_init, test_init, test_pred_init, trainarray.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(features, labels, n_inputs, n_outputs=3):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "        hidden1 = tf.layers.dense(features, n_inputs, name=\"hidden1\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer1)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_inputs, name=\"hidden2\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer2)\n",
    "\n",
    "        #max1 = tf.contrib.layers.maxout(hidden1, n_inputs//2)\n",
    "\n",
    "    #     dropout1 = tf.nn.dropout(hidden1, keep_prob)\n",
    "    #     hidden2 = tf.layers.dense(dropout1, n_hidden2, name=\"hidden2\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer2)\n",
    "    #     dropout2 = tf.nn.dropout(hidden2, keep_prob)\n",
    "    #     hidden3 = tf.layers.dense(dropout2, n_hidden3, name=\"hidden3\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer3)\n",
    "    #     dropout3 = tf.nn.dropout(hidden3, keep_prob)\n",
    "    #     hidden4 = tf.layers.dense(dropout3, n_hidden4, name=\"hidden4\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout4 = tf.nn.dropout(hidden4, keep_prob)\n",
    "    #     hidden5 = tf.layers.dense(dropout4, n_hidden5, name=\"hidden5\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout5 = tf.nn.dropout(hidden5, keep_prob)\n",
    "    #     hidden6 = tf.layers.dense(dropout5, n_hidden5, name=\"hidden6\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout6 = tf.nn.dropout(hidden6, keep_prob)\n",
    "    #     hidden7 = tf.layers.dense(dropout6, n_hidden5, name=\"hidden7\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout7 = tf.nn.dropout(hidden7, keep_prob)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")#, kernel_regularizer=regularizer5)\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    #     l2_loss = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "    #     loss += l2_loss\n",
    "        loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "    learning_rate = 0.002\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "        accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    with tf.name_scope(\"predict\"):\n",
    "        output = tf.nn.softmax(logits)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return training_op, output, loss, accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(features, labels, n_inputs, n_outputs=3):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"cnn\"):\n",
    "        \n",
    "        input_layer = tf.reshape(features, [-1, n_inputs[0], n_inputs[1], 1])\n",
    "                       \n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=input_layer,\n",
    "            filters=32,\n",
    "            kernel_size=[5, n_inputs[1]],\n",
    "            strides=[1,1],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "                       \n",
    "        conv1_flat = tf.reshape(conv1, [-1, n_inputs[0] * n_inputs[1] * 32], name=\"c1flat\")\n",
    "    \n",
    "        hidden1 = tf.layers.dense(conv1_flat, n_inputs[0], name=\"hidden1\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer1)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_inputs[0], name=\"hidden2\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer2)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")#, kernel_regularizer=regularizer5)\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    #     l2_loss = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "    #     loss += l2_loss\n",
    "        loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "    learning_rate = 0.002\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "        accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    with tf.name_scope(\"predict\"):\n",
    "        output = tf.nn.softmax(logits)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return training_op, output, loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(iter_init, test_n, output_op):\n",
    "    with tf.Session() as sess:\n",
    "        preds = np.zeros(test_n)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(iter_init) # drawing samples from test_data\n",
    "        try:\n",
    "            for i in range(test_n):\n",
    "            #for i in range(testarray.shape[0]):\n",
    "                pred = sess.run(output_op) #, feed_dict={keep_prob : 1}) # for dropout only\n",
    "                preds[i] = np.argmax(pred)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print(preds)\n",
    "        \n",
    "        return preds.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test sets\n",
    "trainarray,labelarray,testarray,testlabelarray = split_data(dfX, dfy, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data set to use data windows\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "trainarray, labelarray = features_2d_to_3d(np.array(trainarray), np.array(labelarray), 5)\n",
    "testarray, testlabelarray = features_2d_to_3d(np.array(testarray), np.array(testlabelarray), 5)\n",
    "# trainarray = flatten_3d(trainarray) # don't need to flatten for CNN\n",
    "# testarray = flatten_3d(testarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training\n",
    "batch_size = 100\n",
    "train_n = trainarray.shape[0]\n",
    "test_n = testarray.shape[0]\n",
    "n_batches = train_n // batch_size\n",
    "n_batches_test = test_n // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238840 59707\n"
     ]
    }
   ],
   "source": [
    "# Create datasets & model\n",
    "\n",
    "\n",
    "\n",
    "features, labels, tr_init, te_init, tep_init, n_inputs = create_datasets(trainarray, labelarray, testarray, testlabelarray, batch_size)\n",
    "#training_op, output, loss, accuracy = create_model(features, labels, n_inputs, n_outputs=3)\n",
    "n_inputs = [trainarray.shape[1], trainarray.shape[2]]\n",
    "training_op, output, loss, accuracy = create_cnn_model(features, labels, n_inputs, n_outputs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train_Loss: 41.6706, Test_Loss: 17.2263, Train_Accuracy: 0.9997, Test_Accuracy: 0.9981\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "columns = ['t-plus', 'loss', 'accuracy', 'test_loss', 'test_accuracy']\n",
    "summaries = pd.DataFrame(np.zeros([n_epochs,5], dtype=float), columns=columns)\n",
    "run_name = 'model1'\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #writer = tf.summary.FileWriter('./', tf.get_default_graph())\n",
    "    #writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    tot_batches_run = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(tr_init) # drawing samples from train_data\n",
    "        tot_loss = 0\n",
    "        for i in range(n_batches):\n",
    "            try:\n",
    "                _, loss_value = sess.run([training_op, loss]) # , feed_dict={keep_prob : 0.75} # for dropout only\n",
    "                tot_loss += loss_value\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"out of range on iter {}\".format(i))\n",
    "                break\n",
    "        \n",
    "        \n",
    "        # Now gauge training accuracy\n",
    "        sess.run(tr_init) # drawing samples from test_data\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch = sess.run(accuracy) # , feed_dict={keep_prob : 1} # for dropout only\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        tr_acc = total_correct_preds/train_n\n",
    "        \n",
    "        # Now get testing loss\n",
    "        sess.run(te_init) # drawing samples from test_data\n",
    "        test_tot_loss = 0\n",
    "        for i in range(n_batches_test):\n",
    "            try:\n",
    "                loss_value = sess.run([loss]) # , feed_dict={keep_prob : 0.75} # for dropout only\n",
    "                test_tot_loss += loss_value[0]\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"out of range on iter {}\".format(i))\n",
    "                break\n",
    "        \n",
    "        # Now gauge testing accuracy\n",
    "        sess.run(tep_init) # drawing samples from test_data\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch = sess.run(accuracy) # , feed_dict={keep_prob : 1} # for dropout only\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        te_acc = total_correct_preds/test_n\n",
    "\n",
    "        \n",
    "        epoch_time = time.time()\n",
    "        print(\"Epoch: {}, Train_Loss: {:.4f}, Test_Loss: {:.4f}, Train_Accuracy: {:.4f}, Test_Accuracy: {:.4f}\"\\\n",
    "              .format(epoch, tot_loss, test_tot_loss, tr_acc, te_acc))\n",
    "        cum_time = epoch_time - start_time\n",
    "        summaries.iloc[epoch,:] = cum_time, tot_loss, test_tot_loss, tr_acc, te_acc\n",
    "\n",
    "        #writer.close()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 1. ... 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3679,  2421,  7267],\n",
       "       [10406, 13987,  8587],\n",
       "       [ 3167,  9016,  1177]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "p = get_predictions(tep_init, test_n, output)\n",
    "\n",
    "# actual along left, predicted along top\n",
    "confusion_matrix(testlabelarray, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 1 1 0 1 2 1 0]\n",
      "[2 0 2 0 1 1 2 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(p[:10])\n",
    "print(testlabelarray[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3155911367176378"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(testlabelarray, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
