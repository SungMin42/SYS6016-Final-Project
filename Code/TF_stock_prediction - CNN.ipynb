{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is a script to use deep learning to predict stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and create Dataset iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298558, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data (from pkl file probably)\n",
    "\n",
    "#os.getcwd()\n",
    "df = pd.read_pickle('../data/price_level_total_view_2017-01-03_AAPL_grouped')\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary object to develop to\n",
    "trainarray = np.zeros([10,5,2], dtype=float) #[n_rows, window_size, features]\n",
    "labelarray = np.zeros([10,2], dtype=float) # [n_rows, output_classes]\n",
    "\n",
    "testarray = np.zeros([10,5,2], dtype=float) #[n_rows, window_size, features]\n",
    "testlabelarray = np.zeros([10,2], dtype=float) # [n_rows, output_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_to_3d(data, labels, window):\n",
    "    data_n, data_w = data.shape\n",
    "    stride1, stride2 = data.strides\n",
    "    new_len = data_n - window\n",
    "    data3d = as_strided(data, [new_len , window, data_w], strides=[stride1, stride1, stride2])\n",
    "    return(data3d, labels[:len(labels)-window])\n",
    "\n",
    "def flatten_3d(data):\n",
    "    data_n = data.shape[0]\n",
    "    new_width = data.shape[1]*data.shape[2]\n",
    "    \n",
    "    return np.reshape(data, (data_n, new_width))\n",
    "    \n",
    "def split_data(df, train_frac):\n",
    "    \n",
    "    X = df.iloc[:,0:2]\n",
    "    y = df.iloc[:,2:3]\n",
    "    n = X.shape[0]\n",
    "    cutoff = np.floor(n * train_frac).astype(int) # total - the number you want to test, which here i'm flooring \n",
    "    #                   (amount you want in training should be 1/10th value the denominator)\n",
    "    # cutoff\n",
    "\n",
    "    X_train, X_test = (X.iloc[0:cutoff , :] , X.iloc[cutoff: , :] )\n",
    "\n",
    "    y_train, y_test = (y.iloc[0:cutoff , :].values.ravel() , y.iloc[cutoff: , :].values.ravel() )\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(trainarray, labelarray, testarray, testlabelarray, batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    train_n = trainarray.shape[0]\n",
    "    test_n = testarray.shape[0]\n",
    "\n",
    "    with tf.name_scope(\"dataset\"):\n",
    "        training_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(trainarray, tf.float32),\n",
    "                    tf.cast(labelarray, tf.int32)\n",
    "                )\n",
    "            ).shuffle(buffer_size=2*train_n).batch(batch_size) # multiply by 2 if using accuracy calc\n",
    "        )\n",
    "\n",
    "        test_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(testarray, tf.float32),\n",
    "                    tf.cast(testlabelarray, tf.int32)\n",
    "                )\n",
    "            )\n",
    "        ).shuffle(buffer_size=2*test_n).batch(batch_size)\n",
    "\n",
    "    with tf.name_scope(\"iterator\"):\n",
    "        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n",
    "        features, labels = iterator.get_next()\n",
    "        train_init = iterator.make_initializer(training_dataset) # initializer for train_data\n",
    "        test_init = iterator.make_initializer(test_dataset) # initializer for train_data\n",
    "\n",
    "    return features, labels, train_init, test_init, trainarray.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(features, labels, n_inputs, n_outputs=3):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "        hidden1 = tf.layers.dense(features, n_inputs, name=\"hidden1\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer1)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_inputs, name=\"hidden2\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer2)\n",
    "\n",
    "        #max1 = tf.contrib.layers.maxout(hidden1, n_inputs//2)\n",
    "\n",
    "    #     dropout1 = tf.nn.dropout(hidden1, keep_prob)\n",
    "    #     hidden2 = tf.layers.dense(dropout1, n_hidden2, name=\"hidden2\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer2)\n",
    "    #     dropout2 = tf.nn.dropout(hidden2, keep_prob)\n",
    "    #     hidden3 = tf.layers.dense(dropout2, n_hidden3, name=\"hidden3\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer3)\n",
    "    #     dropout3 = tf.nn.dropout(hidden3, keep_prob)\n",
    "    #     hidden4 = tf.layers.dense(dropout3, n_hidden4, name=\"hidden4\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout4 = tf.nn.dropout(hidden4, keep_prob)\n",
    "    #     hidden5 = tf.layers.dense(dropout4, n_hidden5, name=\"hidden5\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout5 = tf.nn.dropout(hidden5, keep_prob)\n",
    "    #     hidden6 = tf.layers.dense(dropout5, n_hidden5, name=\"hidden6\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout6 = tf.nn.dropout(hidden6, keep_prob)\n",
    "    #     hidden7 = tf.layers.dense(dropout6, n_hidden5, name=\"hidden7\",\n",
    "    #                               activation=tf.nn.relu)#, kernel_regularizer=regularizer4)\n",
    "    #     dropout7 = tf.nn.dropout(hidden7, keep_prob)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")#, kernel_regularizer=regularizer5)\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    #     l2_loss = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "    #     loss += l2_loss\n",
    "        loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "    learning_rate = 0.002\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "        accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    with tf.name_scope(\"predict\"):\n",
    "        output = tf.nn.softmax(logits)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return training_op, output, loss, accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py\n",
    "# will need to set the \n",
    "\n",
    "#with tf.name_scope('convolution')\n",
    "\n",
    "# def create_convolutional_layer(input_data, num_input_channels, conv_filter_size, num_filters = 100):\n",
    "    \n",
    "#     with tf.name_scope('convolution':\n",
    "#         kernel = _variable_with_weight_decay('weights',  \n",
    "\n",
    "#                                          shape=[, 5, 3, 64],\n",
    "\n",
    "#                                          stddev=5e-2,\n",
    "\n",
    "#                                          wd=None)\n",
    "\n",
    "#         conv = tf.nn.conv1d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "#     biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "\n",
    "#     pre_activation = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "#     conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "#     _activation_summary(conv1)\n",
    "\n",
    "def create_cnn_model(features, labels, n_inputs, n_outputs=3):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"cnn\"):\n",
    "                       \n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=features,\n",
    "            filters=32,\n",
    "            kernel_size=[5, n_inputs[1]],\n",
    "            strides=[1,0],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "                       \n",
    "        conv1_flat = tf.reshape(conv1, [-1, n_inputs[0] * n_inputs[1] * 32])\n",
    "    \n",
    "        hidden1 = tf.layers.dense(conv1_flat, n_inputs[0], name=\"hidden1\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer1)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_inputs[0], name=\"hidden2\",\n",
    "                                  activation=tf.nn.elu)#, kernel_regularizer=regularizer2)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")#, kernel_regularizer=regularizer5)\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    #     l2_loss = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "    #     loss += l2_loss\n",
    "        loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "    learning_rate = 0.002\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "        accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    with tf.name_scope(\"predict\"):\n",
    "        output = tf.nn.softmax(logits)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return training_op, output, loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test sets\n",
    "trainarray,labelarray,testarray,testlabelarray = split_data(df, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data set to use data windows\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "trainarray, labelarray = features_2d_to_3d(np.array(trainarray), np.array(labelarray), 5)\n",
    "testarray, testlabelarray = features_2d_to_3d(np.array(testarray), np.array(testlabelarray), 5)\n",
    "# trainarray = flatten_3d(trainarray) # don't need to flatten for CNN\n",
    "# testarray = flatten_3d(testarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training\n",
    "batch_size = 100\n",
    "train_n = trainarray.shape[0]\n",
    "test_n = testarray.shape[0]\n",
    "n_batches = train_n // batch_size\n",
    "n_batches_test = test_n // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 5, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e9636285b694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mn_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrainarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-5017acc9e339>\u001b[0m in \u001b[0;36mcreate_cnn_model\u001b[0;34m(features, labels, n_inputs, n_outputs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             activation=tf.nn.relu)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mconv1_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       _scope=name)\n\u001b[0;32m--> 417\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \"\"\"\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m                            \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m                            str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m   1478\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 5, 2]"
     ]
    }
   ],
   "source": [
    "# Create datasets & model\n",
    "\n",
    "\n",
    "\n",
    "features, labels, tr_init, te_init, n_inputs = create_datasets(trainarray, labelarray, testarray, testlabelarray, batch_size)\n",
    "#training_op, output, loss, accuracy = create_model(features, labels, n_inputs, n_outputs=3)\n",
    "\n",
    "n_inputs = [trainarray.shape[1], trainarray.shape[2]]\n",
    "training_op, output, loss, accuracy = create_cnn_model(features, labels, n_inputs, n_outputs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "columns = ['t-plus', 'loss', 'accuracy', 'test_loss', 'test_accuracy']\n",
    "summaries = pd.DataFrame(np.zeros([n_epochs,5], dtype=float), columns=columns)\n",
    "run_name = 'model1'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #writer.add_graph(sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tot_batches_run = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(tr_init) # drawing samples from train_data\n",
    "        tot_loss = 0\n",
    "        for i in range(n_batches):\n",
    "            try:\n",
    "                _, loss_value = sess.run([training_op, loss]) # , feed_dict={keep_prob : 0.75} # for dropout only\n",
    "                tot_loss += loss_value\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"out of range on iter {}\".format(i))\n",
    "                break\n",
    "        \n",
    "        \n",
    "        # Now gauge training accuracy\n",
    "        sess.run(tr_init) # drawing samples from test_data\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch = sess.run(accuracy) # , feed_dict={keep_prob : 1} # for dropout only\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        tr_acc = total_correct_preds/train_n\n",
    "        \n",
    "        # Now get testing loss\n",
    "        sess.run(te_init) # drawing samples from test_data\n",
    "        test_tot_loss = 0\n",
    "        for i in range(n_batches_test):\n",
    "            try:\n",
    "                loss_value = sess.run([loss]) # , feed_dict={keep_prob : 0.75} # for dropout only\n",
    "                test_tot_loss += loss_value[0]\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"out of range on iter {}\".format(i))\n",
    "                break\n",
    "        \n",
    "        # Now gauge testing accuracy\n",
    "        sess.run(te_init) # drawing samples from test_data\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch = sess.run(accuracy) # , feed_dict={keep_prob : 1} # for dropout only\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        te_acc = total_correct_preds/test_n\n",
    "\n",
    "        \n",
    "        epoch_time = time.time()\n",
    "        print(\"Epoch: {}, Train_Loss: {:.4f}, Test_Loss: {:.4f}, Train_Accuracy: {:.4f}, Test_Accuracy: {:.4f}\"\\\n",
    "              .format(epoch, tot_loss, test_tot_loss, tr_acc, te_acc))\n",
    "        cum_time = epoch_time - start_time\n",
    "        summaries.iloc[epoch,:] = cum_time, tot_loss, test_tot_loss, tr_acc, te_acc\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
