{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is a script to use deep learning to predict stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in data (from pkl file probably)\n",
    "\n",
    "#os.getcwd()\n",
    "df = pd.read_pickle('../data/price_level_total_view_2017-01-03_AAPL_grouped_2')\n",
    "df.head()\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_to_3d(data, labels, window):\n",
    "    data_n, data_w = data.shape\n",
    "    stride1, stride2 = data.strides\n",
    "    l_stride1 = labels.strides[0]\n",
    "    new_len = data_n - window\n",
    "    data3d = as_strided(data, [new_len, window, data_w], strides=[stride1, stride1, stride2])\n",
    "    labels3d = as_strided(labels, [new_len, window], strides=[l_stride1, l_stride1])\n",
    "    return data3d, labels3d\n",
    "\n",
    "def flatten_3d(data):\n",
    "    data_n = data.shape[0]\n",
    "    new_width = data.shape[1]*data.shape[2]\n",
    "    \n",
    "    return np.reshape(data, (data_n, new_width)) # flesh this function out\n",
    "    \n",
    "def split_data(dfX, dfy, train_frac):\n",
    "    X = dfX\n",
    "    y = dfy\n",
    "    n = X.shape[0]\n",
    "    cutoff = np.floor(n * train_frac).astype(int) # total - the number you want to test, which here i'm flooring\n",
    "    #                   (amount you want in training should be 1/10th value the denominator)\n",
    "    # cutoff\n",
    "\n",
    "    X_train, X_test = (X.iloc[0:cutoff , :] , X.iloc[cutoff: , :] )\n",
    "    y_train, y_test = (y.iloc[0:cutoff].values.ravel() , y.iloc[cutoff:].values.ravel() )\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    X_train = ss.transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def batch_data(data, labels, batch_size, n_steps):\n",
    "    windowed_x, windowed_y = features_2d_to_3d(data, labels, n_steps)\n",
    "    \n",
    "    t_steps = data.shape[0]\n",
    "    width = data.shape[1]\n",
    "    n_batches = t_steps // batch_size\n",
    "    remainder = t_steps - (n_batches * batch_size)\n",
    "    new_len = t_steps - remainder\n",
    "    \n",
    "    windowed_x = windowed_x[:new_len]\n",
    "    windowed_y = windowed_y[:new_len]\n",
    "    \n",
    "    x_batches = np.reshape(windowed_x, [-1, batch_size, n_steps, width])\n",
    "    y_batches = np.reshape(windowed_y, [-1, batch_size, n_steps])\n",
    "    y_batches = y_batches[:,:,n_steps-1]\n",
    "    \n",
    "    print(x_batches.shape)\n",
    "    print(y_batches.shape)\n",
    "\n",
    "    return x_batches, y_batches, n_batches, new_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(n_steps, batch_size):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Define parameters for the RNN\n",
    "    n_inputs = 14\n",
    "    n_neurons = 50\n",
    "    n_outputs = 3\n",
    "\n",
    "    # Set up placeholders for input data\n",
    "    X = tf.placeholder(tf.float32, [batch_size, n_steps, n_inputs], name=\"X\")\n",
    "    labels = tf.placeholder(tf.int32, [batch_size], name=\"y\")\n",
    "    is_training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "    \n",
    "    with tf.name_scope(\"rnn\"):\n",
    "        \n",
    "        cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu), output_size=n_outputs)\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "        drop1 = tf.layers.dropout(final_state, training=is_training, rate=0.2)\n",
    "        \n",
    "    with tf.name_scope(\"output\"):\n",
    "        hidden1 = tf.layers.dense(drop1, 10, name=\"hidden1\", activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden1, n_outputs, name=\"output\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "        \n",
    "        print(logits.shape)\n",
    "        print(labels.shape)\n",
    "\n",
    "    # Define the optimizer; taking as input (learning_rate) and (loss)\n",
    "    with tf.name_scope(\"train\"):\n",
    "        learning_rate = 0.001\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Step 6: Define the evaluation metric\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "\n",
    "    # Step 7: Initiate\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Summaries\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        value_ = tf.placeholder(shape=[], name='summary_placeholder', dtype=tf.float32)\n",
    "        tl = tf.summary.scalar('train_loss', value_)\n",
    "        ta = tf.summary.scalar('train_accuracy', value_)\n",
    "        vl = tf.summary.scalar('val_loss', value_)\n",
    "        va = tf.summary.scalar('val_accuracy', value_)\n",
    "    \n",
    "    return training_op, loss, accuracy, init, X, labels, Y_proba, value_, tl, ta, vl, va, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns and scale data\n",
    "dfX = df.reset_index().iloc[:,5:]  #np.delete(np.arange(16), [0,2])\n",
    "dfX2 = df.reset_index().iloc[:,[2,4]]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(dfX2)\n",
    "dfX2 = ss.transform(dfX2)\n",
    "\n",
    "dfX2 = pd.DataFrame(data=dfX2, columns= ['mid_price_log', 'trade_volume_differential'])\n",
    "dfX = pd.merge(dfX2, dfX, left_index=True, right_index=True)\n",
    "\n",
    "dfy = df.reset_index().iloc[:,3]\n",
    "\n",
    "# Create train/test sets\n",
    "trainarray,labelarray,testarray,testlabelarray = split_data(dfX, dfy, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data set to use data windows\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 400\n",
    "x, y, n_batches, n_train = batch_data(trainarray, labelarray, batch_size, window_size)\n",
    "x_val, y_val, n_batches_val, n_test  = batch_data(testarray, testlabelarray, batch_size, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labelarray).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random labels if we want to test accuracy\n",
    "\n",
    "# labelarray = np.asarray([np.random.randint(0,3) for i in np.arange(labelarray.shape[0])])\n",
    "# testlabelarray = np.asarray([np.random.randint(0,3) for i in np.arange(testlabelarray.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create datasets & model\n",
    "\n",
    "training_op, loss, accuracy, init, X, labels, Y_proba, value_, tl, ta, vl, va, is_training = create_rnn_model(window_size, batch_size)\n",
    "\n",
    "#temp = create_datasets(trainarray, labelarray, testarray, testlabelarray, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=50\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('../tf_graphs/', tf.get_default_graph())\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        for b in range(n_batches):\n",
    "            X_batch, y_batch = x[b], y[b]\n",
    "            _, loss_value, acc_value = sess.run([training_op, loss, accuracy], \n",
    "                                                feed_dict={X: X_batch, labels: y_batch, is_training: True})\n",
    "            train_loss += loss_value\n",
    "            train_accuracy += acc_value\n",
    "        train_accuracy = train_accuracy/n_train\n",
    "        \n",
    "        s = sess.run(ta, feed_dict={value_: train_accuracy})\n",
    "        writer.add_summary(s, epoch)\n",
    "        s = sess.run(tl, feed_dict={value_: train_loss})\n",
    "        writer.add_summary(s, epoch)\n",
    "        \n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        for b in range(n_batches_val):\n",
    "            X_batch, y_batch = x_val[b], y_val[b]       \n",
    "            loss_value, acc_value = sess.run([loss, accuracy], feed_dict={X: X_batch, labels: y_batch})\n",
    "            val_loss += loss_value\n",
    "            val_accuracy += acc_value\n",
    "        val_accuracy = val_accuracy/n_test\n",
    "\n",
    "        s = sess.run(va, feed_dict={value_: val_accuracy})\n",
    "        writer.add_summary(s, epoch)\n",
    "        s = sess.run(vl, feed_dict={value_: val_loss})\n",
    "        writer.add_summary(s, epoch)\n",
    "        \n",
    "        print(\"Epoch: {}, Train acc: {:.4f}, Val acc: {:.4f}, Train loss: {:.4f}, Val loss: {:.4f}\".format(epoch, train_accuracy, val_accuracy, train_loss, val_loss))\n",
    "\n",
    "    saver.save(sess, \"./my_time_series_model\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                          \n",
    "    saver.restore(sess, \"./my_time_series_model\")\n",
    "\n",
    "    X_new = x_val\n",
    "    y_pred = sess.run(Y_proba, feed_dict={X: X_new[0]})\n",
    "    \n",
    "y_pred\n",
    "np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
