{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is a script to use deep learning to predict stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in data (from pkl file probably)\n",
    "\n",
    "#os.getcwd()\n",
    "df = pd.read_pickle('../SYS6016-Final-Project/data/price_level_total_view_2017-01-03_AAPL_grouped_2')\n",
    "df.head()\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_to_3d(data, labels, window):\n",
    "    data_n, data_w = data.shape\n",
    "    stride1, stride2 = data.strides\n",
    "    new_len = data_n - window\n",
    "    data3d = as_strided(data, [new_len , window, data_w], strides=[stride1, stride1, stride2])\n",
    "#    return(data3d, labels[:len(labels)-window])\n",
    "    return(data3d, labels[window:])\n",
    "\n",
    "def flatten_3d(data):\n",
    "    data_n = data.shape[0]\n",
    "    new_width = data.shape[1]*data.shape[2]\n",
    "    \n",
    "    return np.reshape(data, (data_n, new_width)) # flesh this function out\n",
    "    \n",
    "def split_data(dfX, dfy, train_frac):\n",
    "\n",
    "   X = dfX\n",
    "   y = dfy\n",
    "   n = X.shape[0]\n",
    "   cutoff = np.floor(n * train_frac).astype(int) # total - the number you want to test, which here i'm flooring\n",
    "   #                   (amount you want in training should be 1/10th value the denominator)\n",
    "   # cutoff\n",
    "\n",
    "   X_train, X_test = (X.iloc[0:cutoff , :] , X.iloc[cutoff: , :] )\n",
    "   y_train, y_test = (y.iloc[0:cutoff].values.ravel() , y.iloc[cutoff:].values.ravel() )\n",
    "\n",
    "   ss = StandardScaler()\n",
    "   ss.fit(X_train)\n",
    "   X_train = ss.transform(X_train)\n",
    "   X_test = ss.transform(X_test)\n",
    "\n",
    "   return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(trainarray, labelarray, testarray, testlabelarray, batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    train_n = trainarray.shape[0]\n",
    "    test_n = testarray.shape[0]\n",
    "\n",
    "    with tf.name_scope(\"dataset\"):\n",
    "        training_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(trainarray, tf.float32),\n",
    "                    tf.cast(labelarray, tf.int32)\n",
    "                )\n",
    "            ).shuffle(buffer_size=2*train_n).batch(batch_size) # multiply by 2 if using accuracy calc\n",
    "        )\n",
    "\n",
    "        test_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                (\n",
    "                    tf.cast(testarray, tf.float32),\n",
    "                    tf.cast(testlabelarray, tf.int32)\n",
    "                )\n",
    "            )\n",
    "        ).shuffle(buffer_size=2*test_n).batch(batch_size)\n",
    "\n",
    "    with tf.name_scope(\"iterator\"):\n",
    "        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n",
    "        features, labels = iterator.get_next()\n",
    "        train_init = iterator.make_initializer(training_dataset) # initializer for train_data\n",
    "        test_init = iterator.make_initializer(test_dataset) # initializer for train_data\n",
    "\n",
    "    return features, labels, train_init, test_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(features, labels, height, width):\n",
    "    \n",
    "    conv1_fmaps, conv1_ksize, conv1_stride, conv1_pad = 32, 2, 1, \"SAME\"\n",
    "    conv2_fmaps, conv2_ksize, conv2_stride, conv2_pad = 64, 2, 1, \"SAME\"\n",
    "\n",
    "    # Define a pooling layer\n",
    "    pool3_dropout_rate = 0.25\n",
    "    pool3_fmaps = conv2_fmaps\n",
    "\n",
    "    # Define a fully connected layer\n",
    "    n_fc1 = 128\n",
    "    fc1_dropout_rate = 0.5\n",
    "\n",
    "    # Output\n",
    "    n_outputs = 3\n",
    "\n",
    "    #tf.reset_default_graph() # this is called instead in Dataset creation\n",
    "\n",
    "    # Step 2: Set up placeholders for input data\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X_reshaped = features\n",
    "        y = labels\n",
    "        training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "    # Step 3: Set up the two convolutional layers using tf.layers.conv2d\n",
    "    # Hint: arguments would include the parameters defined above, and what activation function to use\n",
    "    conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, \n",
    "                             strides=(conv1_stride), padding=conv1_pad, activation=tf.nn.relu,\n",
    "                             name=\"conv1\")\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, \n",
    "                             strides=(conv2_stride), padding=conv2_pad, activation=tf.nn.relu,\n",
    "                             name=\"conv2\")\n",
    "\n",
    "    # Step 4: Set up the pooling layer with dropout using tf.nn.max_pool\n",
    "\n",
    "    #4-d tensor is [batch, height, width, channel] -- no one does pooling across the batch apparently, so keep that 1 always\n",
    "    with tf.name_scope(\"pool3\"):\n",
    "        pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=\"pool\")    \n",
    "        pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * height//2 * width//2])\n",
    "        pool3_flat_drop = tf.layers.dropout(pool3_flat, pool3_dropout_rate, training=training)\n",
    "\n",
    "    # Step 5: Set up the fully connected layer using tf.layers.dense\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "        fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\n",
    "\n",
    "    # Step 6: Calculate final output from the output of the fully connected layer\n",
    "    with tf.name_scope(\"output\"):\n",
    "        # Hint: \"Y_proba\" is the softmax output of the \"logits\"\n",
    "        logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "    # Step 5: Define the optimizer; taking as input (learning_rate) and (loss)\n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Step 6: Define the evaluation metric\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct, accuracy = None, None\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "\n",
    "    # Step 7: Initiate\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    return training_op, loss, accuracy, init, saver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns and scale data\n",
    "dfX = df.reset_index().iloc[:,5:]  #np.delete(np.arange(16), [0,2])\n",
    "dfX2 = df.reset_index().iloc[:,[2,4]]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(dfX2)\n",
    "dfX2 = ss.transform(dfX2)\n",
    "\n",
    "dfX2 = pd.DataFrame(data=dfX2, columns= ['mid_price_log', 'trade_volume_differential'])\n",
    "dfX = pd.merge(dfX2, dfX, left_index=True, right_index=True)\n",
    "\n",
    "dfy = df.reset_index().iloc[:,3]\n",
    "\n",
    "# Create train/test sets\n",
    "trainarray,labelarray,testarray,testlabelarray = split_data(dfX, dfy, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labelarray).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random labels if we want to test accuracy\n",
    "\n",
    "# labelarray = np.asarray([np.random.randint(0,3) for i in np.arange(labelarray.shape[0])])\n",
    "# testlabelarray = np.asarray([np.random.randint(0,3) for i in np.arange(testlabelarray.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data set to use data windows\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "window_size = 6\n",
    "# if window_size isn't even, tf.reshape throws an error down below after pooling\n",
    "trainarray, labelarray = features_2d_to_3d(np.array(trainarray), np.array(labelarray), window_size)\n",
    "testarray, testlabelarray = features_2d_to_3d(np.array(testarray), np.array(testlabelarray), window_size)\n",
    "#trainarray = flatten_3d(trainarray)\n",
    "#testarray = flatten_3d(testarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = trainarray.shape[0]\n",
    "test_n = testarray.shape[0]\n",
    "height = trainarray.shape[1]\n",
    "width = trainarray.shape[2]\n",
    "n_inputs = height*width\n",
    "\n",
    "batch_size = 400\n",
    "n_batches = train_n // batch_size\n",
    "n_batches_test = test_n // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainarray = np.reshape(trainarray, newshape=[-1, height, width, 1])\n",
    "testarray = np.reshape(testarray, newshape=[-1, height, width, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets & model\n",
    "\n",
    "features, labels, tr_init, te_init = create_datasets(trainarray, labelarray, testarray, testlabelarray, batch_size)\n",
    "training_op, loss, accuracy, init, saver = create_model(features, labels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "columns = ['t-plus', 'loss', 'accuracy', 'test_loss', 'test_accuracy']\n",
    "summaries = pd.DataFrame(np.zeros([n_epochs,5], dtype=float), columns=columns)\n",
    "run_name = 'model1'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #writer.add_graph(sess.graph)\n",
    "    sess.run(init)\n",
    "    tot_batches_run = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(tr_init) # drawing samples from train_data\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        for i in range(n_batches):\n",
    "            try:\n",
    "                _, loss_value, acc_value = sess.run([training_op, loss, accuracy]) # , feed_dict={keep_prob : 0.75} # for dropout only\n",
    "                train_loss += loss_value\n",
    "                train_accuracy += acc_value\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"out of range on iter {}\".format(i))\n",
    "                break\n",
    "        train_accuracy = train_accuracy/(train_n)\n",
    "                \n",
    "        # Now get testing loss\n",
    "        sess.run(te_init) # drawing samples from test_data\n",
    "        test_loss, test_accuracy = 0, 0\n",
    "        for i in range(n_batches_test):\n",
    "            try:\n",
    "                loss_value, acc_value = sess.run([loss, accuracy]) # , feed_dict={keep_prob : 0.75} # for dropout only\n",
    "                test_loss += loss_value\n",
    "                test_accuracy += acc_value\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"out of range on iter {}\".format(i))\n",
    "                break\n",
    "        test_accuracy = test_accuracy/(test_n)\n",
    "        \n",
    "        \n",
    "#         epoch_time = time.time()\n",
    "        print(\"Epoch: {}, Train_Loss: {:.4f}, Test_Loss: {:.4f}, Train_Accuracy: {:.4f}, Test_Accuracy: {:.4f}\"\\\n",
    "              .format(epoch, train_loss, test_loss, train_accuracy, test_accuracy))\n",
    "#         cum_time = epoch_time - start_time\n",
    "#         summaries.iloc[epoch,:] = cum_time, train_loss, test_loss, train_accuracy, te_acc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
